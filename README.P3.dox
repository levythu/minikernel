# 15-410/605 P3: Operating System Kernel
Leiyu Zhao (leiyuz)
## Highlight Design Decision

### GlobalLock and LocalLock
For the section that we don't want to be interrupted, we differentiate GlobalLock from Locallock. Locallock is implemented simply by disabling and enabling interrupts, while GlobalLock is a superset of LocalLock that also ensures only one CPU core can enter the session. Since inside both lock interrupts are disabled, on single CPU, GlobalLock degrades to LocalLock. However, we differentiate them to make the kernel easier to be scaled out to multicore.

Another interesting design is that those locks are CPU-reentrant, i.e., for the same CPU, a lock can be acquired arbitrary times without any problem or worrying about when should *real* unlock takes place.

Note that GlobalLock is a spinlock. In multicore environment, it's reasonable to spin sometimes, while in single core GlobalLock never spins. Similarly, there're some spinloops on owning a thread, but it's still only for multicore situation. In single core it always succeeds on the first tick. Since those spinloops to own a thread only happens when a thread is confirmed to be in `THREAD_BLOCK`, no one would really own it, and any temporary own by scheduler (for preflight check) is protected by LocalLock. Therefore, there's no chance for such spinloop to be more than one-operation in uniprocessor machine.

### Kernel Mutex (kmutex)
GlobalLock/LocalLock is used only for small and critical session that we don't want anything to interrupt. But there're great number of cases where we don't have to be so strict: we allow interleaving with other threads that's unrelated to the critical session. In those cases, we use kmutex, it has very similar semantics to user space mutex, and never disable context switch during the section. For overall prreemptibility, it's a good practice to use kmutex when possible.

Fine-grained lock design makes it easy to ensure interrupt and multithread safety while keeping the best preemptibility.

### MACRO-Generated Syscall Stub, Interrupt Handler and Fault Handler
For those who are registered to IDT, we all use macros to generate codes in very short time, to make it quicker to develop and nicer to look. See `fault_handler_internal.h`, `make_syscall_handler.h` for more details.

### On-the-stack WaitList

In any kind of blocking (blocking a thread and wait for some condition), the original thread needs to register itself to some "waitlist" so that the thread that fulfills the condition knows who to wake. Although it's doable through malloc'd linkedlist or fixed length array, in the kernel we put the linkedlist node on the kernel stack of blocked thread. It does not waste extra space: since kernel stack is reserved for usage anyway, and it's safe: as long as the thread is blocked, we know that it will never progress so kernel stack never changes.

### Process & Thread

PCB and TCB are the core data structures in the kernel.  They have complex states and variant transitions. See `process.h` for a very detail description of everything. PCBs and TCBs are organized as linkedlist, while there's an extra XLX (express links) for quick access to runnable threads.

Ephemeral access (weak reference) is introduced to prevent a referred PCB/TCB from disappearing from memory unexpectedly. It's required when you are not sure about its state (as supposed to the cases when you own the thread, or you are the alarm of the thread), and need to release it when you own it, or don't need it. Ephemeral access is just a reference count, which may defer the destroying of PCB/TCB even if it's freed by reaper or waiter.

### Scheduler + Reaper

Scheduler in the kernel is a simple round-robin version. It uses XLX to access all runnable threads, (plus dead threads to reap). Scheduler will run in the following situation:

- Timer fires
- Deschedule happens (user calls deschedule)
- Current thread blocks for any reason
- Yield happens (user calls yield(-1))

Reaper run inside scheduler to reap a dead thread in different kernel stack, so that the kernel stack of reaped thread can be safely freed. Also, reaper is responsible for turning a process into zombie. Although sounds like an entity, reaper is never run as a dedicated threads. Actually, when reaper is working, it's fully preemptive and the interrupt handler can even run another instance of reaper stacked on the original one.

The only situation that reaper will by scheduler is in deschedule mode. (case 2 and 3) In deschedule mode we know that the current thread will not come back for a long time, so we hope this thread to do nothing before being descheduled. Otherwise, the reaper may be hanged halfway and introduce extra wait time for waiter.

### Physical Memory Management ZFOD Design

All free physical pages index are kept in kernel memory as a stack, and each time user request memory, one is popped to make a mapping in page table. 

Compared to the normal pages, ZFOD pages work in a more interesting way: it involves two-phase supply. 

Since ZFOD pages don't require a real physical page until writes happens, when physical memory manager is asked for a ZFOD page, it simply returns a pre-defined all-zero common page, and the caller should register this as a read-only mapping. Meanwhile, pm manager *reserve* a spare physical page in the stack from bottom, to avoid over-provisioning blocks. This is the first phase.

The second phase takes place when page fault happens due to writes on fake ZFOD page. In the situation, kernel ask pm manager to "cash out" the reserved page, and from that point on, the page has nothing special than normal pages any more.

### Safety Checkpoint

All codes have bugs, and we can be vigilant to it, panic the kernel before it introduces much more weird misbehavior. Safety checkpoint happens everywhere, and the common and important checkpoints are:

- All lock infra, to ensure that we never unlock twice, use unmatched rwlock, etc.
- Interrupt bit check on mode switch, to ensure that we never disable interrupt unexpectedly due to incomplete LocalLock nests.
- Kernel stack corruption check on common core functions and interrupt handler, to avoid kernel stack overflow (maybe due to too high timer rate)

### Misbehave

Although we don't implement misbehave to adjust the way kernel behaves, we use misbehave as a special syscall entry to peep into the kernel at runtime. Supported usage:

#### Misbehave 701

This will dump all kernel states as a user readable tree (in simics console). In stable states in the shell, running `misbehave 701` will get the following status:

```
LevyOS Kernel Status──────────────────────────────────
├ Physical Memory Tracker
│ ├ Total User Memory Page: 61182
│ ├ ZFOD User Memory Page: 0
│ └ Available User Memory Page: 61097
├ Process List
│ ├ [   4 <-    3] (RUN) fTID:4, nThr:1, wCh:0
│ ├ [   3 <-    2] (RUN) fTID:3, nThr:1, wCh:0
│ ├ [   2 <-    1] (RUN) fTID:2, nThr:1, wCh:0
│ ├ [   1 <-   -1] (RUN) fTID:1, nThr:1, wCh:1
│ └ Total 4 processes
├ Thread List
│ ├ [   4 <-    4] (RUN) isOwned:T
│ ├ [   3 <-    3] (BLK) isOwned:F
│ ├ [   2 <-    2] (BLK) isOwned:F
│ ├ [   1 <-    1] (W8T) isOwned:F
│ └ Total 4 threads
├ Express Links
│ ├ Threads #4
│ ├ Threads #1
│ └ Total 2 threads
├ CPU Info
│ ├ CPU ID: 0
│ ├ Running TID:    4
│ ├ Running PID:    4
│ ├ Interrupt: OFF
│ └ Current #LocalLock: 0
├ Kernel Memory Allocator
│ └ Bytes allocated: 397384
└─────────────────────────────────────────────────────

```

Physical Memory Tracker helps detect user-space memory leak, while Kernel Memory Allocator helps detect kernel-space memory leak. Process and Thread List help greatly in finding and debugging problems related to thread and process lifetime (fork, vanish, context switch, etc.)

For the latest stable version we cannot find abnormal numbers after running solidity tests and stability tests several times with timer rate = 1000/s.

## Known Bugs
- In Readline, backspace can remove characters on the screen that does not belong to this round of readline

## Future Refinement
- On print, when there're scrolls, it's slow and therefore improper to be put in synchronous timer event. A better solution is to design 2-phase-print, where the 1st phase only logs things to put and 2nd phase puts them. Then timer synchronous event only does the 1st phase while asynchronous event does the 2nd.
